* General
** Libraries
*** Apache Xalan
**** TODO These are no longer used. Remove from this file
**** See LICENSE.txt in the binary download at
***** http://www.whoishostingthis.com/mirrors/apache/xalan/xalan-j/binaries/xalan-j_2_7_2-bin.zip
**** Apache 2
***** xalan.jar
***** xercesImpl.jar
***** xml-apis.jar
**** Public domain
***** xml-apis.jar
****** yes this jar is part public domain and part Apache 2.
*** Apache Tika
**** Extracts text from files with different mime types
**** Apache 2
**** https://tika.apache.org/download.html
*** jpingy
**** https://code.google.com/p/jpingy/downloads/detail?name=jpingy0_1-alpha.jar&can=2&q=
**** MIT License
***** http://www.opensource.org/licenses/mit-license.php
*** Apache Commons
**** Lang
***** http://commons.apache.org/proper/commons-lang/download_lang.cgi
**** IO
***** https://commons.apache.org/proper/commons-io/download_io.cgi
**** Apache 2
*** Lucene
**** http://www.whoishostingthis.com/mirrors/apache/lucene/java/4.10.1/
**** Apache 2
**** Jars
***** lucene-analyzers-common-4.10.1.jar
***** lucene-expressions-4.10.1.jar
***** lucene-core-4.10.1.jar
***** lucene-facet-4.10.1.jar
***** lucene-queries-4.10.1.jar
***** lucene-queryparser-4.10.1.jar
*** Jackson
**** Download links
***** http://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.4.2/jackson-core-2.4.2.jar
***** http://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.4.2/jackson-annotations-2.4.2.jar
***** http://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.4.2/jackson-databind-2.4.2.jar
**** Apache 2
*** Snowball
**** Download
***** http://snowball.tartarus.org/index.php
**** part compiler and we distribute the output
***** http://snowball.tartarus.org/dist/snowball_code.tgz
**** part library
***** http://snowball.tartarus.org/dist/libstemmer_java.tgz
**** BSD licence
***** http://snowball.tartarus.org/license.php
*** Schinke
**** http://snowball.tartarus.org/otherapps/schinke/intro.html
**** Schinke R, Greengrass M, Robertson AM and Willett P (1996) A stemming algorithm for Latin text databases. Journal of Documentation, 52: 172-187.
**** We distribute Snowball-compiled version
**** Licence unspecified
*** jdeferred
**** https://github.com/jdeferred/jdeferred
**** Apache 2
*** AsyncHttpClient
**** http://www.whoishostingthis.com/mirrors/apache//httpcomponents/httpasyncclient/binary/httpcomponents-asyncclient-4.0.2-bin.zip
**** Apache 2
** Data queries
*** primary key seems to be eprintid
**** DONE confirm
     CLOSED: [2014-10-03 Fri 10:18]
***** They can be used as primary keys
*** Not all works have a poem_text_3
**** eg eprintid: 3810, 4602, 4681
***** Skip if not latin
***** Otherwise use titles if no text
* Meeting on 08/10/2014
** Points
*** Cleaning up of source data
**** Unescaped double quotes in strings -> escape double quotes
**** Unescaped tabs -> spaces
**** unescape occurrences -> preprocessor replaces with escaped version of arg (although tabs are replaced with spaces)
**** DONE Send changes upstream
     CLOSED: [2014-10-09 Thu 10:58]
*** Lucene provides ngram tokenisers and filters
**** For stemming maybe have a look at stempel - a universal rule-based stemmer
*** Return to discussion about stemming prior to searching Google with trigrams
**** My argument is we shouldn't stem since anything indexed by Google won't have been stemmed
*** Licensing
** Nick is to look at web search api
*** Google is out
*** He'll look at Bing and Yahoo (BOSS)
** Queries
*** Case in Latin -- does it have the same significance / lack of significance as in English
**** Currently everything is lowercased
**** ie is it okay to downcase?
**** Can be cases where it's significant eg: Dominus
***** Must ask mark (TODO is below)
*** Do trigrams across punctuation (,;.[]!) make sense?
**** For identifying an extract of a conductus poem these might be particularly important trigrams as they most likely represent a juxtaposition of more than one distinct concept. Conversely a trigram contained within a single clause would possibly only represent one concept, and therefore would not a distinct 'fingerprint' of the work that contains it.
*** Latin stopwords?
**** First step
***** http://wiki.digitalclassicist.org/Stopwords_for_Greek_and_Latin
**** For web search stage
***** Depends on whether exact match search can be specified to web search api I think
* 16/10/2014
** DONE Verify that index is correct
   CLOSED: [2014-10-10 Fri 19:20]
*** Iterate though
** DONE Start work on Stemmer
   CLOSED: [2014-10-10 Fri 19:20]
*** Look at Stempel
**** Forget it, requires training data -- went with Schinke algorithm instead
***** Here's training data -- just need to reformat it to lemma variant1 variant2 etc... format
****** Save output from these to github repository
***** cat <(sed -nr ../../treebank/index_thomisticus/IT-TB_13-10-2014_CONLL-PML-PLS/CONLL/005_SCG_Libri-1\&2.conll -e "s/[0-9]+\t([^\t]+)\t([^\t]+).*$/\\2 \\1/p"|tr '[:upper:]' '[:lower:]') <(grep ../../treebank/perseus_treebank/1.5/data/*.xml -e "lemma" -h|tr '[:upper:]' '[:lower:]'|sed -nr -e "s/^.* form=\"([^\"]*)\" lemma=\"([^\"]*)\".*$/\\2 \\1/p") | grep -v "[^a-z0-9 ]" | sort -u|sed -nr -e "s/^(.*) (.*)$/\\1\\n\\2/p"
****** can't use this though since the same stem won't be used for a given term across both collections
***** sed -nr ../../treebank/index_thomisticus/IT-TB_13-10-2014_CONLL-PML-PLS/CONLL/005_SCG_Libri-1\&2.conll -e "s/[0-9]+\t([^\t]+)\t([^\t]+).*$/\\2 \\1/p"|tr '[:upper:]' '[:lower:]'| grep -v "[^a-zA-Z0-9 ]" |sort -u|sed -nr -e "s/^(.*) (.*)$/\\1\\n\\2/p"
****** 2619 distinct lemmas
****** 8638 distinct variations
***** grep ../../treebank/perseus_treebank/1.5/data/*.xml -e "lemma" -h|tr '[:upper:]' '[:lower:]'|sed -nr -e "s/^.* form=\"([^\"]*)\" lemma=\"([^\"]*)\".*$/\\2 \\1/p"| grep -v "[^a-zA-Z0-9 ]" |sort -u|sed -nr -e "s/^(.*) (.*)$/\\1\\n\\2/p"
****** 6170 distinct lemmas
****** 14001 variations
** Git hub link
*** DONE Add Nick as contributer
    CLOSED: [2014-10-24 Fri 10:03]
** DONE Ask Mark if case in Medieval Latin can be treated as English (i.e. downcase everything)
   CLOSED: [2014-10-16 Thu 13:56]
*** Okay to down case
*** First check if it's possible to match terms with different cases in Lucene -- that way we can have the best of both worlds
** Points
** Added stoplist la.stop from
*** http://sourceforge.net/projects/perseus-hopper/files/perseus-hopper/hopper-20110527/hopper-source-20110527.tar.gz/download in the perseus project
**** http://www.perseus.tufts.edu/hopper
** Stemmer
*** Compiled Schinke with snowball
**** Download from http://snowball.tartarus.org/dist/snowball_code.tgz
**** Uses java classes from http://snowball.tartarus.org/dist/libstemmer_java.tgz rather than their equivalents bundled with Lucene
***** Lucene classes are for a different version and won't compile as is
**** Also copied and modified Lucene's SnowballAnalyzer and SnowballFilter to ensure they use the classes from snowball.tartarus.org and not those bundled with Lucene
**** Schinke stemer generates two stems
***** The automatically generated version of LatinStemmer applied noun stemming first and then applied verb stemming, overwriting the noun stem in the process
***** I modified LatinStemmer to allow the type of stemming to be specified (i.e. as either VERB stemming, NOUN stemming or UNKNOWN in which case the shorter stem was returned or verb stem where both had the same length)
***** UNKNOWN was set as the default as this resulted in the smallest index and manual inspection of output revealed satisfactory results (a bit wishy washy -- I know)
***** Obviously an alternative approach is to apply part-of-speech tagging -- I'd be curious to know if it led to more accurate stemming than our current default method.
**** DONE Read Schinke paper
     CLOSED: [2014-10-15 Wed 18:27]
**** DONE Mail Nick with stemming update
     CLOSED: [2014-10-15 Wed 18:27]
**** DONE Is unspecified Schinke licence an issue?
     CLOSED: [2014-10-16 Thu 14:20]
***** It's BSD
*** Hunpos might be an option if we're not happy with Stempel
**** https://code.google.com/p/hunpos/
** Using query expansion to increase recall at the web-search stage
*** ie use terms to top N web pages return in response to a query
*** not for now
** Why trigrams?
*** Don't really need to use tri-grams if we have a way to rank bigrams and unigrams
**** Ideally we don't want to have to page results so reducing number of results by searching for trigrams helps
** Cleaning up source data
*** Any tabs in original json have now been escaped correctly (as \t)
** While perusing index
*** Vast majority of tri-grams have the same TFIDF due tri-gram only occurring once in the collection
*** TF calcualtion
**** Currently we use a simple count of terms in a poem
**** TODO Would like to account for poem length
***** Consider later
***** ie normalise tf for length && also account for term repetition in poems due to repeated lines / chorus
*** IDF calculation
**** Don't worry about this till we know how many bi-grams /tri-grams
**** Currently calculated based on total number of poems and number of poems containing a particular trigram
***** Which is good for weighting those trigrams which can be used to distinguish one poem in the collection from the others in conductus
***** What we want is to distinguigh a poem from irrelevant pages on the web
****** Therefore idf calculation should be based on total number of documents indexed by search engine and total number of web-pages containing trigram
******* Web api can probably return number of documents containing trigram
******** (and if no web pages are returned this trigram can be skipped altogether)
******* We also need total number of indexed pages -- alternatively use estimate by searching for disjunction of English stop-words?
****** This will also (hopefully) result in a greater variety of tf-idfs
** DONE Eliminate tri-grams with digits. Typically they correspond to stanza / verse numbers.
   CLOSED: [2014-10-16 Thu 17:38]
*** Check digit aren't used elsewhere
**** I'll have a better idea of the best way to do this after working with the stemmer and by extension TokenFilters
** Queries
*** Which licence on github
**** Apache if possible -- not fussy
*** ToDos
**** DONE Send sempel output to Mark and the lads
     CLOSED: [2014-10-23 Thu 11:54]
**** DONE Get no of distinct bi-grams / tri-grams / terms
     CLOSED: [2014-10-16 Thu 16:21]
***** Provide list on bi-grams if possible (ordered by frequency)
***** Terms will result in multiple pages so might not be practical from price point of view
***** Trigrams
****** all: 71416
****** removed _: 42229
***** Bigrams
****** all: 55954
****** removed _: 46489
***** Unigrams
****** all: 15019
****** removed _:15019
**** DONE Familiarise myself with Bing search api
     CLOSED: [2014-10-30 Thu 10:30]
* 30/10/2014 14h00
** Stopwords in ngrams might make sense
*** Depending on language a verb - preposition bigram could increase the preciseness of the query
**** eg in English "speak of" and "speak to" mean too different things
**** I've these currently enabled -- this changes the unumber of distinct tokens of course
*** For now keep stop words in
**** TODO check it's okay later
** The role of stemming
**** Including all known morphological forms of the three words within a trigram (assuming trigrams are used) could result in a query which is too large for Bing to process
***** Upper limit seems to be 2047 chars, including path in url
****** http://stackoverflow.com/questions/15334531/what-are-the-query-length-limits-for-the-bing-websearch-api
***** Although browser based search seems limited to 10 words (anymore are ignored)
****** Unsure if this is relevant to api though
**** So initially I intended to submit only different morphological variations of each ngram
***** But these were very few in number
***** Only accomplished the same as not having stemming to begin with
**** Conversely expanding each term to each possible morphological variant and doing this for each term in an ngram results in a large number of queries and very long queries that must be split up it we wish to include all of them
***** 3-grams
****** 64553 queries
******* 1391593 conjunctions
******* ie ~22 disjunctions per query
***** 2-grams
****** 61230 queries
******* 392109 conjunctions
******* ie ~6 disjunctions per query
***** An issue here is we'll be sending many queries in succession, 
****** many returning no results 
****** and the results they do return may contain duplicated results as a result of there being multiple similar queries
**** A third option is to prioritise permutations and only include as many disjunctions as fit
***** First include those that occur in the text
***** Then rank the remainder by multiplying frequency of each term
** Blacklisted sites
*** Ideally I'd like to eliminate these from the initial search in Bing
**** It'll save us downloading these links again and again. Some of these blacklisted urls point to longish documents not only wasting bandwidth but increasing the likelihood that desired links time out due to the time spending downloading these links
*** Variations of NOT site: and NOT domain: failed
*** Considering using NOT in combination with keywords or phrases
**** Might be an idea to confirm that these are actually sufficiently distinctive - we don't want to filter out more sites than specified by Gregorio
***** DONE Does NOT site: work in api
      CLOSED: [2014-11-04 Tue 11:23]
****** Yes it does. Not only but the Bing api also accepts a path as well as a domain as an argument to site: 
******* Encountered a site (in archive.org) that Gregorio might want to blacklist. Unfortunately I never took note of what it was exactly
******** DONE There are bound to be more so it'd be nice if these could be blacklisted at a later stage too.
	 CLOSED: [2014-11-21 Fri 00:09]
********* I believe this site was one of the archive.org/stream sites. I contacted Gregorio about this. See below.
**** DIAMM for DIAMM
***** Entered this phrase into Bing browser interface and any links returned that I checked either related to this TML or to something completely off topic
**** MUSICARUM LATINARUM for TML
***** Entered this phrase into Bing browser interface and any links returned that I checked related to this TML
**** Analecta hymnica for archive.org
***** DONE I'm suspicious that this will filter out too many sites
      CLOSED: [2014-11-21 Fri 13:08]
****** Replaced this blacklisted phrase with three
******* "Galler Schule Processionshymnen dichten"
******* "Binnenreime betrachtet werden k6nnten"
******* "CANT10NE8 ET MUTETE"
******* Each only returns one link in Bing's browser api -- the full text link of the URL I'm trying to exclude so that's okay.
******* Note the apparent OCR error in two of these phrases
**** Conductus
***** Cantum pulcriorem invenire
****** No longer exclude this as conductus URLs don't seem to be returned by Bing (or at least its browser interface)
******* Sought a quoted trigram from conductus and conductus was not in results even when is specified site:catalogue.conductus.ac.uk
******* Repeated with another trigram and conductus wasn't returned either
**** DONE do this
     CLOSED: [2014-10-30 Thu 18:55]
** Points
*** Need bing subscription
*** commas in text do not result in a _ inserted in trigram
*** Download size:
**** assume 64553 queries and 5 hits per query 300K web page size
***** => 1576 MB for queries
***** => 92.344 GB files
**** Ssh into college computer with file space and sufficient network capacity
***** Nick is sorting this out
*** Meeting at end of month with everyone
**** Nick is gong to mail Mark about this
*** DONE query lads about feedback of stemming
    CLOSED: [2014-10-30 Thu 17:34]
* 7/11/2014 10h00
** Points
*** Downloading
**** Eg out of 93 queries there were 59 which had results
**** DONE Be sure of when '_' term are generated by Shingle before submitting full run to Bing
     CLOSED: [2014-11-04 Tue 10:56]
***** '_' are generated anytime a term was removed from underlying stream (e.g. a stopword or numeral if stopwords or numerals respectively are removed)
***** Since we are filtering out trigrams containing '_' chars, the total number of trigrams sent to Bing is now 64524
**** DONE Verify that the longest queries are accepted by Bing before doing full run
     CLOSED: [2014-11-03 Mon 16:48]
***** They weren't accepted. Long queries returned a HTTP Not Found error (strangely).
***** In the end I set maximum query length to 2000. Largest known working length was 2007, but I haven't checked all queries (obviously -- given my limited query budget).
**** bing_queries.txt has fewer lines than trigrams_stemmed_freqs.txt even after removing lines with _ characters
***** DONE Why?
      CLOSED: [2014-11-03 Mon 14:45]
****** Queries at end were omitted due to mishandled end-of-pipe detection
**** DONE why is the serialised stem group file different for unigrams, bigrams and trigrams
     CLOSED: [2014-11-03 Mon 15:46]
***** Stem groups are generated from unstemmed trigrams. Currently we do not allow partial trigrams. Consequently there are fewer distinct terms when indexing with trigrams compared to indexing with (for example) unigrams.
**** Issues with downloads
***** Dynamic content
****** Youtube links
******* nunc sancte nobis
******** http://www.youtube.com/watch?v=wxLJxHKaDu0
********* changing comments
****** Scribed
******* tibi+cogor+obsequi
******** http://www.scribd.com/doc/193904560/Analecta-Hymnica-Medii-Aevi-January-1-1895
******* There is a proper match to the first trigram on this page, however not in the html you download
****** academia.edu
******* same story as Scribed
******** sola+mederis+morte
********* http://www.academia.edu/2638762/Josef_TRUHLAR_O_staroceskych_dramatech_velikonocnich
****** Other sites with databases
******* nunc sancte nobis
******** http://cantusbohemiae.cz/
********* recently added chants are constantly being updated
****** Presumeably trigram was present when indexed
***** Some results give links with dodgy encoding prevents instantiating as a URL instance
****** Ignore as it seems to be working now (only explanation I can think of is that now I use URL constructor initially rather than URI constructor)
****** Original problem described below
******* problem is either in what they're sending or the httpclient library
******* Wireshark is no good because its encrypted and I can't seem to disable encryption
******* 5 or the first 50 hits had this issue
******** seems high but not when you consider the query: a a e
****** Problem resurfaced when I started using CloseableHttpAsyncClient
******* Ignoring for now as problem seems to be quite when dealing with most trigrams (a a e what the specific trigram that caused trouble)
***** Including sites blacklist sometimes seems to affect which results are returned
****** quam dulces remedium
****** Sans blacklist the 4th result (of 11) was 
******* https://auramundi.wordpress.com/category/ars-antiqua/perotinus-magister/
******** This does contain the trigram quam dulces remedium
******* With a blacklist this link is completely missing. 12 links were returned
******* On closer inspected I discovered the following:
******** I searched for "quam dulce remedium" with only one site (catalogue.conductus.ac.uk) blacklisted ie:
********* https://api.datamarket.azure.com/Bing/SearchWeb/Web?Query='("quam dulce remedium") AND (NOT site:catalogue.conductus.ac.uk)'
******** One site was returned: http://catalogue.conductus.ac.uk/ so apparently in this case the blacklist wasn't working
********* DONE Downloading from sites with forms or ajax content (eg http://catalogue.conductus.ac.uk/)
	  CLOSED: [2014-11-19 Wed 14:56]
********** Don't worry about this as the URL wasn't returned in response to the query, but because it thought I was searching for the terms catalogue conductus ac uk
*********** I know this becuse putting in gibberish for the trigram (ie "quadfsdfdsm dulce remedium") also returned the same site
***** adding an extra disjunction leads to a result being removed -- should never happen (two other results are added but that's beside the point)
****** "vita gaudia nos" OR "vita gaudio nos" OR "vitam gaudia nos" -- no blacklist. Results
******* http://archive.org/stream/analectahymnicam4647drev/analectahymnicam4647drev_djvu.txt
****** "vita gaudium nos" OR "vita gaudia nos" OR "vita gaudio nos" OR "vitam gaudia nos" -- no blacklist. Results:
******* http://www.archive.org/stream/patrologiaecurs119unkngoog/patrologiaecurs119unkngoog_djvu.txt
******* http://www.archive.org/stream/patrologiaecurs119unkngoog/patrologiaecurs119unkngoog_djvu.txt&q=video+xxx+de+maria+stola&ei=uLK2T-LOCOO-0QXizPXRBw&sa=X&ct=res&resnum=3&ved=0CBsQFjAC
***** Some links don't contain the exact trigram or even all terms in the trigram
****** eg calore+nec+mutat
****** 36 links returned and one seems possibly relevant -- the 36th
******* http://archive.org/stream/deartemedicalibr01hoev/deartemedicalibr01hoev_djvu.txt
****** In browser Bing will relax the query so that individual terms in phrase anywhere in doc will match
******* Don't think this is what is happening as otherwise we'd have fewer queries with no downloads at all
******* Also from playing with browser, this query relaxation doesn't seem to occur when using boolean query
****** On StackExchange (http://stackoverflow.com/questions/5696666/bing-search-match-only-exact-literal-strings) there was a suggestion to prefix every term with +
******* Note this since our 1st result (http://www.flickr.com/photos/fiore_barbato/15779717492/) was still the first result after making this change
****** DONE Wonder if it's related to the page changing since indexing -- I'll look for cache page
       CLOSED: [2014-11-14 Fri 15:20]
******* Bing web page shows page was changed 11hrs previously. I downloaded my links after that
******** Couldn't downoad original page due to flickr objecting to view page in a frame
******* The relaxation of the query seems to be triggered by the query length.
******** When I removed only the first 2 morphological variations we only got the one good link back
********* Adding one of these back results in all the dodgy resulting being returned
******** When I removed only the second and third last morphological variations (remember the morphological variation that matched the good result was the last one) I got the one good link back
********* Adding one of these back resulted in all the dodgy results being returned once again.
******** In conclusion I'd suggest keeping things as they are.
********* I considered limiting query length somehow but this would have led to us missing out on the successful morphological variation at the end
****** Also similar dei+piissima+cuius 
****** Also cute quasi pro
******* Only two results when we applied keyword blacklist
******** One of which did not contain either tri-gram in query
********* However it was a Latin text and matched two consecutive terms from trigram
******** Removing one disjunction
********* Single actual matching result returned
********* Or (removing other disjunction) no results
***** Slow downloads can result in a timeout before completion
****** occurred 2/743 download
***** Timeouts (>3 mins in somecases)
****** http://www.agenziaradicale.com/?id=2808:avant-retro-opere-inutili-di-un-uomo-inutile-mostra-di-giuseppe-vittorio-scapigliatiindex.php/rassegnaweb/
******* Ignoring as browser can't download it either
******* for totum+traxit+tonans
****** http://www.cairn.info/revue-archives-d-histoire-doctrinale-et-litteraire-du-moyen-age-2005-1-page-105.htm
******* for hec+est+fides
******* Worked with browser and problem went away after retrying on a subsequent run
****** http://ldysinger.stjohnsem.edu/@magist/1930_Pius11/Pi11%20casti-connubii-Lat-Engl.doc
******* for proles+et+hominis
******* Didn't even work for browser (but that got a HTTP 404). Subsequent run didn't work either.
***** Other failed downloads
****** http://www.academia.edu/4107017/%D0%AE%D0%B1%D0%B8%D0%BB%D0%B5%D0%B9_%D0%B8_%D1%8E%D0%B1%D0%B8%D0%BB%D0%B5%D0%B8._%D0%AE%D0%91%D0%98%D0%9B%D0%95%D0%99_%D0%98_%D0%AE%D0%91%D0%98%D0%9B%D0%95%D0%98_%D0%A3%D0%9D%D0%98%D0%92%D0%95%D0%A0%D0%A1%D0%90%D0%9B%D0%AC%D0%9D%D0%90%D0%AF_%D0%98%D0%94%D0%95%D0%AF_%D0%98_%D0%9B%D0%9E%D0%9A%D0%90%D0%9B%D0%AC%D0%9D%D0%90%D0%AF_%D0%A0%D0%98%D0%9C%D0%A1%D0%9A%D0%90%D0%AF_%D0%98%D0%A1%D0%A2%D0%9E%D0%A0%D0%98%D0%AF to journal/partial/33._%D0%AE%D0%91%D0%98%D0%9B%D0%95%D0%99_%D0%98_%D0%AE%D0%91%D0%98%D0%9B%D0%95%D0%98_%D0%A3%D0%9D%D0%98%D0%92%D0%95%D0%A0%D0%A1%D0%90%D0%9B%D0%AC%D0%9D%D0%90%D0%AF_%D0%98%D0%94%D0%95%D0%AF_%D0%98_%D0%9B%D0%9E%D0%9A%D0%90%D0%9B%D0%AC%D0%9D%D0%90%D0%AF_%D0%A0%D0%98%D0%9C%D0%A1%D0%9A%D0%90%D0%AF_%D0%98%D0%A1%D0%A2%D0%9E%D0%A0%D0%98%D0%AF
******* Filename too long due to long filename and period and the beginning of it
******* in et+cessandi+propositum
****** URL without www prefix caused a problem. Could ping and download when I added www. prefix
****** http://www.gutenberg.org/files/17859/17859-h/files/colloquium1.html
******* in silentium+deus+in
******* http 403
****** http://www.ex.ua/get/4508132
******* Unknown error (in DeferredZeroCopyConsumer)
****** http://www.itweb.co.za/index.php?option=com_content&view=article&id=139211
******* Unknown error
****** http://www.archive.org/stream/anessayonorigin01crokgoog/anessayonorigin01crokgoog_djvu.txt
******* in velut+torrentem+lacrimas
****** Incorrectly encoded URL in results
******* DONE eg http://documentacatholicaomnia.eu/03d/0354-0430,_Augustinus,_Sermones_[5]_de_Diversis_(Serm._341-396),_LT.doc
       	CLOSED: [2014-11-11 Tue 17:59]
****** Cooke rejected warning from org.apache.http.client.protocol.ResponseProcessCookies processCookies
**** Possible additions to blacklisted sites:
***** http://www.archive.org/stream/analectahymnica21drevuoft/analectahymnica21drevuoft_djvu.txt
***** I contacted Gregorio about this (on 13/11/2014)
****** He agreed that I could add these archive.org/stream variations of the archive.org URL he listed and remove the originals (as they never seemed to be returned)
**** I considered eliminating blacklist (at least while downloading)
***** up to 16 of 298 queries I tried might be affected
****** ie they returned 100 results and so might have missed out interesting urls due to presence of blacklisted links
***** In more depth (the following queries has 100 results)
****** query -- why aren't I finding more blacklisted urls?
****** vult+et+quotquot
******* 2 blacklisted urls (tml)
****** theone+vel+in
******* 0 blacklisted urls
****** superne+syon+filie
******* 0 blacklisted urls
******* results in general here seem to be of a poor quality
****** sola+gratia+o
******* 0 blacklisted urls
******* results in general also seem to be rubbish
****** salute+non+manibus
******* 0 blacklisted urls
****** quod+in+virginali
******* 0 blacklisted urls
****** pro+me+mori
******* 0 blacklisted urls
**** Estimate of download size (now superceded by better estimate below)
***** Range of 400-600MB
****** 400
******* ((608/93)*64524)/1024
******** 608MB (smalleset download size)
******** 93 queries (largest number of queries)
****** 600
******* ((693/73)*64524)/1024
******** 693 (largest download size)
******** 73 (smalleset number of queries)
***** Assumed normal distribution of size
****** Mean 638.33333MB
****** Std deviation 47.43768
****** 693, 614, 608 total download sizes
****** P(X<748.690)==99.0
**** Many links downloaded are from archive.org 
***** 9/45
**** Added heartbeat to kill downloading processes if another website (ie google.com) becomes unreachable.
#  LocalWords:  unstemmed unigrams serialised bigrams Bing txt
*** Indexing downloads
**** No longer provide mime type provided by remote server to Tika as sometimes it's incorrect
***** eg journals/random_no_blacklist_300_234567_journal/completed/cursum+vite+me/1.SGM
***** TODO find study demonstrating accuracy of Tika's automagical filetype detection algorithm
***** TODO How long will it take to index and how much space is required
****** 11m07s to index 298 queries (from journals/random_no_blacklist_300_234567_journal/)
******* full collection is based on downloads for 64553 queries (ie 216.62 times as many)
******** assuming nlog_2(n) growth rate => 12.97 days
********* assuming branching factor of 2
********** http://comments.gmane.org/gmane.comp.db.cassandra.user/7693
****** 309MB to index 298 queries
******* Assuming linear growth 65GBs will be required for index
*** In the end due to unpredictability of Bing I decided against including blacklist in queries submitted to search engine
**** Especially in light of above examination of 16 queries of 298 where full complement of 100 results were returned
*** Added file deduplication
*** Re-indexed CPI collection after I realised I had originally ommitted the refrains
**** The refrains were listed in the refrain_text_3 field and not with the main body of the text in the poem_text_3 field.
**** Note each refrain is only listed once -- not after each stanza as might the case if displayed
***** good -- they won't distort our n-gram frequencies used in generating boolean queries for search engine
*** New size requirements estimate
**** Note file deduplication has reduced estimated file size somewhat
**** 785 queries --- 5.1GB
**** 65418 queries--- 425GB
**** + index
***** 785 queries --- 1.7GB
***** 65418 --- 142GB
***** Assuming linear growth.
***** I expect growth is asymptotic in reality but with a limit that is much larger than our current index size
****** => eventual index size should be a little smaller
**** Total size 567GB
*** New time estimate
**** Indexing growth rate won't be linear, rather it'll be nlogn
**** => indexing will take somewhat longer
**** Assuming log_2 from http://comments.gmane.org/gmane.comp.db.cassandra.user/7693
**** For download and indexing
**** 785 queries --- 2.2hrs
**** 65418 queries --- 184.7hrs linear component broken down as
***** 162.54 + 22.164 indexing
****** accounting for log growth
****** = 164.54 + 36.87
**** = ~184.7 + 36.87hrs
**** = 201.41 hrs or 8.39 days assuming other factors don't change
***** If indexer falls behind downloader to the point where it no longer benefits from file-cache things could slow down dramatically


