* Tools
** See libraries in meetings.org
*** Mention we didn't use schinke stemmer
** See other resources in meetings.org
* Methods
** Train Stempel stemmer on the perseus treeback data.
** Generate trigrams from CPI export
*** no stoplist was used when generating trigrams.
*** The Conductus was parsed and 
** Query Bing
*** Series of Boolean queries, one for each distinct stemmed trigram in the conductus.
*** No need to send trigrams containing digits -- these were invariably stanza / verse numbers
*** Retrieved up to 100 results for each query (configurable)
**** Concurrency of download / indexing
***** Download thread performed asynchronous download of all results of a given query
****** Did not proceed to next query until results of previous query were all downloaded
***** Had two download threads, so if one thread was held up by a single slow download the other could still proceed
***** Single threaded indexer
****** Did not want downloads to run to far ahead of indexer so I limited the number of download threads
******* It'd be nice if download were throttled if indexer fell too far behind
**** Remember to retain source URLs
**** Remember to retain mime info
***** for displaying the downloaded file correctly
***** to know how to extract textual content from file
****** admittedly I relied on Tika library to guess file types
******* sometimes mime info provided my server can be incorrect
******** eg text/plain was the mime type returned for http://tera-3.ul.cs.cmu.edu/NASD/d23d381a-642a-4cb1-bd42-5373f518ed1d/lemur/3115.sgml
********* an SGML file
*** Each query contained one quoted trigram and as many morphological variants (as determined by stem groups generated from Latin conductus poems and titles) of the terms within those trigrams as would fit in a valid Bing query.
**** Max length of Bing API query is 2047 according to http://stackoverflow.com/questions/15334531/what-are-the-query-length-limits-for-the-bing-websearch-api
***** If you hit the limit you get HTTP 404 Not Found error
***** However even allowing for the fact that + chars are converted to %20 the actual limit seem lower
***** I set an actual limit of 2000s and all queries tested so far seem to work.
****** Might be possible to increase it up a bit more.
*** Did not filter out blacklisted websites at this stage
**** TODO List actual blacklisted sites later
**** Initially included clauses to filter out these sites
***** First by using a negated sites: clause
****** eg clauses such as... AND NOT site:chmtl.indiana.edu/tml
****** This doesn't work in the Bing web interface but does seem to work via the REST API (mostly)
******* However sometimes Bing seemed treat the blacklisted site as a search term and returned it instead
******* eg the query '("quam dulce remedium") AND (NOT site:catalogue.conductus.ac.uk)' returned http://catalogue.conductus.ac.uk/
***** Secondly by including negated terms that effectively distinguish blacklisted site from others
****** eg AND NOT "MUSICARUM LATINARUM"
***** However both of these approaches take up the space in the query reducing number of morphological variations that can be sent
****** Eliminating at this stage incurs black incurs a very small cost as out of 298 queries present to bing
******* Only 16 resulted in a full 100 results being returned (therefore no valid pages should be omitted due to slots occupied by blacklisted sites)
******** Only a small minority of these returned any blacklisted sites at all (ie 1 query out of 7 examined).
*** Obviously going to problems with dynamically generated pages
**** eg youtube, 
**** And sites where displayed content might be relevant but wasn't available inline with the downloaded page
***** eg scribed,
***** academie.edu
**** but didn't worry about these
*** Bing inconsistencies
**** Sometimes adding a (desired) disjunction for a trigram removed a valid result from the list returned
**** Sometimes many of the top ranking results returned didn't seem to fit the boolean query at all
***** Sometimes these seems to be dynamically generated content such as youtube pages (with all the latest comments)
****** Didn't mind these -- maybe matching trigram were present when the page was indexed
***** But sometime even unrelated static pages were returned at the top of the results relegating actual matching pages
****** These seem to be ads
*** Unusable Bing results
**** Some URLs returned by Bing had a character set encoding couldn't be handled by Apache's HTTPAsyncClient
***** Skipped these are they were rare and tended to be returned in response to uninteresting trigrams (eg "a a e")
**** Also non-URL encoded urls
***** http://documentacatholicaomnia.eu/03d/0354-0430,_Augustinus,_Sermones_[5]_de_Diversis_(Serm._341-396),_LT.doc
***** No foolproof way to handle all of these (URL encoding is used for a reason after all)
** Report generator
*** JQuery mobile library to create a nice presentable / usable report
**** JQuery Mobile loads all pages at once
***** With ~900 poems (one page per poem) that's a lot of work and can be slow in some browsers
***** Lazily created new iframe for results anytime new poem was opened rather than creating many iframes at start to speed initial opening
****** Even so, report is very slow to navigate in Internet Explorer
* Materials
* Issues / Problems
** When parsing the CPI export initially I forgot about the Refrains as I wasn't aware the field exists since they're only present in some poems
** CPI export
*** Daniel's XML export is the one you should use since, unlike the JSON export it retrains new line character information for poems
**** I used the JSON export for everything bar report generation as I only discovered this to be an issue this in the day.
***** new line character information is only required for report generation (so poems can be displayed sensibly).
*** Also JSON export had problem with how certain characters were quoted and had to be preprocessed before it could be parsed successfully
**** eg tab characters
** Deployment
*** Time to download / index
**** Handling of timeouts when downloading
***** don't want one large slow download to hold everything up
****** max interval between packets received
******* 10 seconds
****** connect timeout
******* 10 seconds
****** completion timeout
******* 180 seconds
****** alleviate problem of large slow downloads by only download any given URL once at most if possible
**** Integrated indexing with downloading to speed things up
***** Found that crawling filesystem, reading downloaded documents from disk to very slow
***** Better index them straight after downloading, while they're still cached in RAM somewhere
**** Gracefully handle crashes
***** Ensure you don't need to start from scratch after a crash
****** Implemented query-by-query transaction journal
**** Network issues
***** Google.com heartbeat
**** Total estimated time requirement: 8-9 days
***** assuming no crashes and that indexer can keep up with download and therefore benefit from not needing to read from disk
*** Storage requirement
**** 65418 trigrams, top 100 results (if present)
**** Total for documents and index 567GB
***** Estimated space requirement: 425GB
***** Estimated index space: 142GB
**** Deduplication important here
*** Time to generate report
**** 
*** Report size
**** I selected top 100 documents returned
***** for line concordances this is more that sufficient, probably
***** Multithreaded download pitfall
****** I had two threads downloading files simultaneously
****** Each thread was unaware of which files the other had downloaded
****** Therefore there will be 2 copies of many files
****** The second copy of each file can be filtered out at report generation time
******* However each duplicate returned will still occupy one of the 100 documents returned in response to a query
******** So in the worst case scenario you're only going to get the top 50 documents
***** Blacklisting 
****** URLs from the following sites were not displayed in the report
******* catalogue.conductus.ac.uk
******* diamm.ac.uk
******* chmtl.indiana.edu/tml
******* archive.org/stream/analectahymnicam20drev
******* archive.org/details/analectahymnicam20drev
******* archive.org/stream/analectahymnica21drevuoft
******* archive.org/details/analectahymnica21drevuoft
******* archive.org/stream/analectahymnicam21drev
******* archive.org/details/analectahymnicam21drev
****** Filtering out blacklisted sites in javascript provided flexibility
******* More can be added without regenerating report
******* However any filtered blacklisted site still occupied one of the 100 results, possibly at the cost of losing a result you're interested in (ranked below those 100 docs)
*** Running from server or locally
**** Security issues
***** The issue with both of these is that I have downloaded many documents automatically without being able to vet them to check for malicious content
****** When running locally you need to disable same origin restrictions 
****** If hosted on a server would the server admin want to take responsibility for this content? (rhetorical -- I expect the answer to be no)
***** Maybe some sort of (signed?) disclaimer would fix this
**** Display of locally stored content (ie file:// urls)
***** Sometimes the extension of a downloaded file does not match its actual mime-type
***** When displaying such a file the browser doesn't know what the mime type is and therefore the file is displayed incorrectly
***** Eg http://www.archive.org/stream/cantionesetmute00churgoog/cantionesetmute00churgoog_djvu.txt
****** It's a html file but if the local copy is opened it is display as a  text file (ie with tags visible)
***** This could be fixed by hosting content on a server but a server side script would still need to provide the mime type
* What Alan wants
** to identify 
*** general issues
*** lessons
*** points of contact
** they should address at the meeting in Jan and subsequently
*** in order to maximise benefits of different strands of the transforming musicology project to each other
** What can people at Goldsmiths and QMUL (the technical people strand) do to benefit our project.
